from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader
import streamlit as st
from dotenv import load_dotenv
import tempfile
import os

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()                                    # åŠ è½½.envæ–‡ä»¶

# è®¾ç½®é¡µé¢æ ‡é¢˜å’Œå¸ƒå±€
st.set_page_config(page_title="ç®€æ˜“RAGæ–‡æ¡£é—®ç­”ç³»ç»Ÿ", layout="wide")    # è®¾ç½®é¡µé¢æ ‡é¢˜å’Œå¸ƒå±€
st.title("ğŸ“„ ç®€æ˜“RAGæ–‡æ¡£é—®ç­”ç³»ç»Ÿ")                                # è®¾ç½®é¡µé¢æ ‡é¢˜

# åˆå§‹åŒ–session state
if "processed_files" not in st.session_state:                   # åˆå§‹åŒ–æ˜¯å¦å¤„ç†è¿‡æ–‡ä»¶
    st.session_state.processed_files = False
if "vectorstore" not in st.session_state:                       # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
    st.session_state.vectorstore = None
if "messages" not in st.session_state:                         # åˆå§‹åŒ–èŠå¤©å†å²
    st.session_state.messages = [
        {"role": "assistant", "content": "æ‚¨å¥½ï¼Œæˆ‘å¯ä»¥å›ç­”å…³äºæ‚¨æ–‡æ¡£çš„é—®é¢˜ï¼Œæˆ–è€…è¿›è¡Œä¸€èˆ¬æ€§å¯¹è¯ï¼"}]

# æ–‡ä»¶ä¸Šä¼ ç»„ä»¶
uploaded_files = st.sidebar.file_uploader(
    "ä¸Šä¼ TXTæ–‡æ¡£",
    type=["txt"],
    accept_multiple_files=True
)

# å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶
if uploaded_files:                                                           # å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶
    with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£ï¼Œè¯·ç¨å€™..."):                                   # æ˜¾ç¤ºåŠ è½½åŠ¨ç”»
        docs = []                                                             # ä¿å­˜å¤„ç†åçš„æ–‡æ¡£
        temp_dir = tempfile.TemporaryDirectory()                              # åˆ›å»ºä¸´æ—¶ç›®å½•

        for file in uploaded_files:                                             # éå†ä¸Šä¼ çš„æ–‡ä»¶
            # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
            temp_filepath = os.path.join(temp_dir.name, file.name)            # ä¿å­˜æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
            with open(temp_filepath, "wb") as f:                              # ä¿å­˜æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
                f.write(file.getvalue())                                     # ä¿å­˜æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•

            # ä½¿ç”¨TextLoaderåŠ è½½æ–‡æœ¬æ–‡ä»¶
            loader = TextLoader(temp_filepath, encoding="utf-8")             # åŠ è½½æ–‡æœ¬æ–‡ä»¶
            docs.extend(loader.load())                                       # åŠ è½½æ–‡æ¡£

        # æ–‡æ¡£åˆ†å‰²
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=100
        )
        splits = text_splitter.split_documents(docs)

        # åˆ›å»ºå‘é‡æ•°æ®åº“
        embeddings = HuggingFaceEmbeddings()

        st.session_state.vectorstore = FAISS.from_documents(
            documents=splits,                                         # æ–‡æ¡£åˆ†å‰²
            embedding=embeddings                                     # ä½¿ç”¨HuggingFaceEmbeddingsè¿›è¡Œå‘é‡åŒ–
        )

        st.session_state.processed_files = True
        st.success("æ–‡æ¡£å¤„ç†å®Œæˆï¼")

# æ˜¾ç¤ºèŠå¤©å†å²
for msg in st.session_state.messages:                                       # æ˜¾ç¤ºèŠå¤©å†å²
    st.chat_message(msg["role"]).write(msg["content"])                      # æ˜¾ç¤ºèŠå¤©å†å²

# èŠå¤©è¾“å…¥æ¡† - æ— è®ºæ˜¯å¦ä¸Šä¼ æ–‡ä»¶éƒ½å¯ä»¥æé—®
user_query = st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...")                                # èŠå¤©è¾“å…¥æ¡†

if user_query:                                                              # å¤„ç†ç”¨æˆ·è¾“å…¥
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    st.session_state.messages.append({"role": "user", "content": user_query})   # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    st.chat_message("user").write(user_query)                                   # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯

    with st.chat_message("assistant"):                                         # æ˜¾ç¤ºæœºå™¨äººæ¶ˆæ¯
        with st.spinner("æ­£åœ¨æ€è€ƒ..."):                                          # æ˜¾ç¤ºåŠ è½½åŠ¨ç”»
            if st.session_state.processed_files and st.session_state.vectorstore:    # å¤„ç†è¿‡æ–‡ä»¶
                # ä½¿ç”¨RAGæ¨¡å¼ï¼ˆæœ‰ä¸Šä¼ æ–‡æ¡£ï¼‰
                try:
                    # åˆ›å»ºLLM
                    llm = ChatOpenAI(
                        model="deepseek-chat",
                        base_url='https://api.deepseek.com/v1',
                        temperature=0.1
                    )

                    # åˆ›å»ºè®°å¿†æ¨¡å—
                    memory = ConversationBufferMemory(
                        return_messages=True,
                        memory_key='chat_history',
                        output_key='answer'
                    )

                    # åˆ›å»ºæ£€ç´¢å™¨å’Œé“¾æ¡
                    retriever = st.session_state.vectorstore.as_retriever(search_kwargs={'k': 3})
                    chain = ConversationalRetrievalChain.from_llm(
                        llm=llm,
                        memory=memory,
                        retriever=retriever,
                        return_source_documents=True
                    )

                    # è°ƒç”¨é“¾æ¡
                    response = chain.invoke({
                        'question': user_query,
                        'chat_history': []
                    })

                    answer = response['answer']

                except Exception as e:
                    answer = f"å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™ï¼š{str(e)}"
            else:
                # ä½¿ç”¨æ™®é€šå¯¹è¯æ¨¡å¼ï¼ˆæ— ä¸Šä¼ æ–‡æ¡£ï¼‰
                try:
                    llm = ChatOpenAI(
                        model="deepseek-chat",
                        base_url='https://api.deepseek.com/v1',
                        temperature=0.7
                    )
                    response = llm.invoke(user_query)
                    answer = response.content
                except Exception as e:
                    answer = f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™ï¼š{str(e)}"

            # æ˜¾ç¤ºç­”æ¡ˆ
            st.write(answer)
            st.session_state.messages.append({"role": "assistant", "content": answer})